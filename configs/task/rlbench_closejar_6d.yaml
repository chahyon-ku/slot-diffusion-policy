# Image dataset

name: close_jar_image

image_shape: &image_shape [3, 128, 128]
shape_meta: &shape_meta
  # acceptable types: rgb, low_dim
  obs:
    # Should be consistent with the rgb views found in the training set
    front_rgb:
      shape: *image_shape
      type: rgb
    wrist_rgb:
      shape: *image_shape
      type: rgb
    # left_shoulder_rgb:
    #   shape: *image_shape
    #   type: rgb
    # right_shoulder_rgb:
    #   shape: *image_shape
    #   type: rgb
    state:
      shape: [10]
      type: low_dim
  action:
    shape: [10]

env_runner:
  _target_: slot_diffusion_policy.rlbench_image_runner.RlbenchImageRunner
  shape_meta: *shape_meta
  action_mode:
    _target_: rlbench.action_modes.action_mode.MoveArmThenGripper
    arm_action_mode:
      # _target_: rlbench.action_modes.arm_action_modes.EndEffectorPoseViaPlanning
      _target_: rlbench.action_modes.arm_action_modes.EndEffectorPoseViaIK
    gripper_action_mode:
      _target_: rlbench.action_modes.gripper_action_modes.Discrete
  obs_config:
    _target_: rlbench.observation_config.ObservationConfig
    left_shoulder_camera:
      _target_: rlbench.observation_config.CameraConfig
      rgb: False
      depth: False
      mask: False
    right_shoulder_camera:
      _target_: rlbench.observation_config.CameraConfig
      rgb: False
      depth: False
      mask: False
    overhead_camera:
      _target_: rlbench.observation_config.CameraConfig
      rgb: False
      depth: False
      mask: False
    wrist_camera:
      _target_: rlbench.observation_config.CameraConfig
      rgb: True
      depth: False
      mask: False
      # image_size: [128, 128]
    front_camera:
      _target_: rlbench.observation_config.CameraConfig
      rgb: True
      depth: False
      mask: False
      # image_size: [128, 128]
  n_train: 5
  n_train_vis: 5
  train_start_seed: 0
  n_test: 5
  n_test_vis: 5
  legacy_test: True
  test_start_seed: 10000
  max_steps: 300
  n_obs_steps: ${n_obs_steps}
  n_action_steps: ${n_action_steps}
  fps: 10
  past_action: ${past_action_visible}
  n_envs: null
  rgbd: False
  rot_6d: True

dataset:
  _target_: slot_diffusion_policy.rlbench_image_dataset.RlbenchImageDataset
  # Make sure this is consistent with the dataset you want to be using
  # zarr_path: data/rlbench_128/train/close_jar/variation00/data.zarr
  # zarr_path: data/rlbench_128/train/close_jar/variation00/data.zarr2
  zarr_path: data/rlbench_128/train/close_jar/variation00/rgbd.zarr
  horizon: ${horizon}
  pad_before: ${eval:'${n_obs_steps}-1'}
  pad_after: ${eval:'${n_action_steps}-1'}
  seed: 42
  val_ratio: 0 # Set this to 0 for now since the eval environment hasn't been set up yet
  max_train_episodes: 90
  # Should be consistent with the rgb views found in the training set
  rgb_views: ['front_rgb', 'wrist_rgb'] #['front_rgb', 'left_shoulder_rgb', 'right_shoulder_rgb']
  rgbd: False
  rot_6d: True
